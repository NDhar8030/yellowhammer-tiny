{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c442a97-a911-43ec-a1a0-c6a19d21f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import tensorflow_io as tfio\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb339997-3087-45db-9c32-0ecb5bfb87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    log_loss,\n",
    "    auc,\n",
    "    average_precision_score\n",
    ")\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "143c9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3428871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664a9bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM:     34.29 GB\n",
      "Available RAM: 19.29 GB\n",
      "Used RAM:      15.00 GB\n",
      "RAM Usage:     43.8%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total RAM:     {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available RAM: {mem.available / 1e9:.2f} GB\")\n",
    "print(f\"Used RAM:      {mem.used / 1e9:.2f} GB\")\n",
    "print(f\"RAM Usage:     {mem.percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6843f",
   "metadata": {},
   "source": [
    "## Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5da31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\foxir\\OneDrive\\Desktop\\Smart Rock\\yellowhammer-tiny\\notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d173bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NEG = 'Development_Set//Training_Set//Negatives'\n",
    "train_POS = 'Development_Set//Training_Set//Yellowhammer'\n",
    "val_NEG = 'Development_Set//Validation_Set//Negatives'\n",
    "val_POS = 'Development_Set//Validation_Set//Yellowhammer'\n",
    "\n",
    "train_neg = [[train_NEG+'//'+item,0] for item in os.listdir(train_NEG)]\n",
    "train_pos = [[train_POS+'//'+item,1] for item in os.listdir(train_POS)]\n",
    "val_neg = [[val_NEG+'//'+item,0] for item in os.listdir(val_NEG)]\n",
    "val_pos = [[val_POS+'//'+item,1] for item in os.listdir(val_POS)]\n",
    "\n",
    "full_train = train_neg+train_pos\n",
    "full_val = val_neg+val_pos\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(full_train)\n",
    "random.shuffle(full_val)\n",
    "\n",
    "t_filepaths = [x[0] for x in full_train]\n",
    "t_labels = [x[1] for x in full_train]\n",
    "v_filepaths = [x[0] for x in full_val]\n",
    "v_labels = [x[1] for x in full_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e065d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL = 'Evaluation_Set'\n",
    "eval = [EVAL+'//'+item for item in os.listdir(EVAL)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc63be1",
   "metadata": {},
   "source": [
    "## Initialize Audio Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5211289",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_sr = 16000\n",
    "\n",
    "@tf.function\n",
    "def load_wav_16k_mono_tf(filename, label):\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    # resample wav if sample rate is not as desired\n",
    "    wav = tf.cond(tf.not_equal(sample_rate, desired_sr),\n",
    "                  lambda: tfio.audio.resample(wav, rate_in=sample_rate, rate_out=desired_sr),\n",
    "                  lambda: wav)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    return wav, label\n",
    "\n",
    "#@tf.function\n",
    "def load_wav_16k_mono_tf_nolabel(filename):\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    #print(sample_rate)\n",
    "    #print(len(wav))\n",
    "    #print(len(wav)/sample_rate)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    # resample wav if sample rate is not as desired\n",
    "    wav = tf.cond(tf.not_equal(sample_rate, desired_sr),\n",
    "                  lambda: tfio.audio.resample(wav, rate_in=sample_rate, rate_out=desired_sr),\n",
    "                  lambda: wav)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "frame_length = 256\n",
    "frame_step = 192 # 75%\n",
    "mel_bins = 64\n",
    "fmax = 8000\n",
    "fmin = 1028\n",
    "\n",
    "@tf.function\n",
    "def preprocess_mel_db_tf(wav, label):\n",
    "\n",
    "    # pad wav if too short else keep it as initial length\n",
    "    wav_len = tf.shape(wav)[0]\n",
    "    wav = tf.cond(wav_len > 32000,\n",
    "                  lambda: wav,\n",
    "                  lambda: tf.cond(wav_len < 32000,\n",
    "                                  lambda: tf.concat([wav, tf.zeros([32000 - wav_len], dtype=tf.float32)], axis=0),\n",
    "                                  lambda: wav))\n",
    "\n",
    "    # create the spectrogram\n",
    "    spectrogram = tfio.audio.spectrogram(wav, frame_length, frame_length, frame_step)\n",
    "    mel_spectrogram = tfio.audio.melscale(spectrogram, sample_rate, mel_bins, fmin, fmax)\n",
    "    dbscale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\n",
    "\n",
    "    # add a dimension for Conv2D input --not needed if augmenting the spectrogram\n",
    "    #dbscale_mel_spectrogram = tf.expand_dims(dbscale_mel_spectrogram, axis=2)\n",
    "\n",
    "    return dbscale_mel_spectrogram, label\n",
    "\n",
    "@tf.function\n",
    "def preprocess_mel_db_tf_nolabel(wav):\n",
    "\n",
    "    # pad wav if too short and cut it if too long\n",
    "    wav_len = tf.shape(wav)[0]\n",
    "    wav = tf.cond(wav_len > 32000,\n",
    "                  lambda: wav,\n",
    "                  lambda: tf.cond(wav_len < 32000,\n",
    "                                  lambda: tf.concat([wav, tf.zeros([32000 - wav_len], dtype=tf.float32)], axis=0),\n",
    "                                  lambda: wav))\n",
    "\n",
    "    # create the spectrogram\n",
    "    spectrogram = tfio.audio.spectrogram(wav, frame_length, frame_length, frame_step)\n",
    "    mel_spectrogram = tfio.audio.melscale(spectrogram, sample_rate, mel_bins, fmin, fmax)\n",
    "    dbscale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\n",
    "\n",
    "    # add a dimension for Conv2D input --not needed if augmenting the spectrogram\n",
    "    #dbscale_mel_spectrogram = tf.expand_dims(dbscale_mel_spectrogram, axis=2)\n",
    "\n",
    "    return dbscale_mel_spectrogram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yellowhammer_tiny",
   "language": "python",
   "name": "yellowhammer_tiny"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
